{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.max_rows', 1000)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "# plt.style.use('fivethirtyeight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "education = pd.read_csv(r'B:\\_DataBGTRes\\Doctoral_Data\\doc_education_info_with_indicator.csv')\n",
    "print(len(education))\n",
    "print(education['BGTResID'].nunique())\n",
    "\n",
    "jobs = pd.read_csv(r'B:\\_DataBGTRes\\Doctoral_Data\\doc_job_info.csv')\n",
    "print(len(jobs))\n",
    "print(jobs['BGTResID'].nunique())\n",
    "\n",
    "education = education.convert_dtypes()\n",
    "jobs = jobs.convert_dtypes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "education.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(education[education['ind_doc']==1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(float('nan'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Isolate degrees with '#' in any of the fields\n",
    "\n",
    "columns = ['MajorCipCode', 'major', 'degreeLevel', 'CompletionDateRaw']\n",
    "\n",
    "edu = education.copy()\n",
    "\n",
    "def pounds(string):\n",
    "    string = str(string)\n",
    "    return len(string.split('#')) - 1\n",
    "        \n",
    "for column in columns:\n",
    "    edu[column + '_pound'] = edu[column].apply(pounds)\n",
    "\n",
    "docs = edu[edu['ind_doc']==1]\n",
    "\n",
    "docs_no_pound = None\n",
    "first = True\n",
    "for column in columns:\n",
    "    if(first):\n",
    "        docs_no_pound = docs[docs[column + '_pound']==0]\n",
    "        first = False\n",
    "    else:\n",
    "        docs_no_pound = docs_no_pound[docs_no_pound[column + '_pound']==0]\n",
    "        \n",
    "docs_pound = docs.drop(docs_no_pound.index)\n",
    "\n",
    "print(len(docs_pound), len(docs_no_pound))\n",
    "\n",
    "print(docs_pound['BGTResID'].nunique(), docs_no_pound['BGTResID'].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_columns = ['MajorCipCode_pound', 'major_pound', 'degreeLevel_pound', 'CompletionDateRaw_pound']\n",
    "possible_pound_combinations = docs_pound[docs_pound['ind_doc']==1].groupby(group_columns).count()[['BGTResID']]\n",
    "# display(possible_pound_combinations)\n",
    "\n",
    "# Iterate over the aggregate\n",
    "empty = True\n",
    "output = None\n",
    "\n",
    "# Temporary consumable DataFrame for speed advantage\n",
    "df = docs_pound.copy()\n",
    "\n",
    "# Iterate over all combinations of pound symbol divisions in the dataset\n",
    "for index, row in possible_pound_combinations.reset_index().iterrows():\n",
    "    nums = set()\n",
    "    safe = True\n",
    "    \n",
    "    # Collect the unique values for number of pound symbols\n",
    "    for i in range(len(group_columns)):\n",
    "        nums.add(row[i])\n",
    "    \n",
    "    # If there are 3 or more of them, then the columns cannot be disentangled\n",
    "    if len(nums) > 2:\n",
    "        safe = False\n",
    "        \n",
    "    # If there are exactly two unique values, at least one must be zero to be disentangled\n",
    "    elif len(nums) == 2:\n",
    "        if ((nums.pop() != 0) & (nums.pop() != 0)):\n",
    "            safe = False\n",
    "               \n",
    "    # If all of the numbers of divisions are the same OR if some of them have no divisons\n",
    "    if safe:\n",
    "        # Find all of the rows in the target table with this valid set of numbers\n",
    "        columns = row.index\n",
    "        temp = df[(df[columns[0]]==row[0])&(df[columns[1]]==row[1])&(df[columns[2]]==row[2])&(df[columns[3]]==row[3])]\n",
    "        \n",
    "        # Put them in a DataFrame together\n",
    "        if empty:\n",
    "            output = temp\n",
    "            empty = False\n",
    "        else:\n",
    "            output = pd.concat([output, temp])\n",
    "\n",
    "        # Throw out all observations that have been approved so they need not be compared to again\n",
    "        df = df.drop(temp.index)\n",
    "        \n",
    "del(df)\n",
    "print(len(output))\n",
    "print(output['BGTResID'].nunique())\n",
    "display(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "output.groupby(group_columns).count()[['BGTResID']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_columns = ['MajorCipCode_pound', 'major_pound', 'degreeLevel_pound', 'CompletionDateRaw_pound']\n",
    "docs_pound[(docs_pound['MajorCipCode_pound']==1)&(docs_pound['major_pound']==2)&(docs_pound['degreeLevel_pound']==3)&(docs_pound['CompletionDateRaw_pound']==2)&(~docs_pound['GPA'].isnull())].iloc[:, :12][:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_no_pound.groupby(group_columns).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "docs_pound.groupby(group_columns).count()[['BGTResID']].sort_values('BGTResID', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################\n",
    "# Purely iterative approach\n",
    "# DON'T USE\n",
    "############################\n",
    "\n",
    "# empty = True\n",
    "# output = None\n",
    "\n",
    "# for index, row in docs_pound.iterrows():\n",
    "#     i = 13\n",
    "#     nums = list()\n",
    "#     prev = None\n",
    "#     for k in range(4):\n",
    "#         prev = row[i+k]\n",
    "#         if prev not in nums:\n",
    "#             nums.append(prev)\n",
    "#     if len(nums) > 2:\n",
    "#         continue\n",
    "#     elif len(nums) == 2:\n",
    "#         if(nums[0] != 0 | nums[1] != 0):\n",
    "#             continue\n",
    "#     else:\n",
    "#         if empty:\n",
    "#             output = pd.DataFrame(row).transpose()\n",
    "#             empty = False\n",
    "#         else:\n",
    "#             output = pd.concat([output, pd.DataFrame(row).transpose()])\n",
    "\n",
    "# print(len(output))\n",
    "# display(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "################################\n",
    "# Completion Date work\n",
    "################################\n",
    "\n",
    "dates = education.copy()\n",
    "\n",
    "def pound_split(string):\n",
    "    if string is not pd.NA:\n",
    "        return string.split('#')\n",
    "    else: \n",
    "        return pd.NA\n",
    "    \n",
    "def num_dates(dates):\n",
    "    if dates is not pd.NA:\n",
    "        return len(dates)\n",
    "    else:\n",
    "        return pd.NA\n",
    "    \n",
    "def remove_unicode_escapes(string):\n",
    "    if string is pd.NA:\n",
    "        return pd.NA\n",
    "    if string is type(float):\n",
    "        return float('nan')\n",
    "    \n",
    "    r_str = string.encode('unicode_escape')\n",
    "    loc = r_str.find(b'\\\\')\n",
    "    \n",
    "    if loc == -1:\n",
    "        return string\n",
    "    \n",
    "    if loc + 1 < len(string):\n",
    "        output = string.replace(string[loc]+string[loc+1], ' ')\n",
    "    else:\n",
    "        output = string.replace(string[loc], '')\n",
    "    \n",
    "#     output = ''\n",
    "#     for s in string.split(string[loc]):\n",
    "#         if output == '':\n",
    "#             output = s\n",
    "#         else:\n",
    "#             output = output + ' ' + s\n",
    "        \n",
    "    return remove_unicode_escapes(output)\n",
    "    \n",
    "dates['CompletionDateProc'] = dates['CompletionDateRaw'].apply(remove_unicode_escapes)\n",
    "\n",
    "single_dates = dates[dates['CompletionDateProc'].apply(pound_split).apply(num_dates)==1]\n",
    "\n",
    "single_dates[single_dates['CompletionDateProc']!=single_dates['CompletionDateRaw']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_dates[single_dates['CompletionDateProc']!=single_dates['CompletionDateRaw']][:1]['CompletionDateRaw'].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def parse_slash_date(string):\n",
    "    if string is pd.NA:\n",
    "        return pd.NA\n",
    "    \n",
    "    string = string.replace(',', '')\n",
    "    string = string.replace('.', '')\n",
    "    string = string.replace('?', '')\n",
    "    \n",
    "    segments = string.split('/')\n",
    "    \n",
    "    # Ignore all strings that are not composed entirely of numbers inside the slashes\n",
    "    for s in segments:\n",
    "        try:\n",
    "            int(s)\n",
    "        except(ValueError):\n",
    "            return pd.NA\n",
    "    \n",
    "    l = len(segments)\n",
    "    \n",
    "    day = None\n",
    "    month = None\n",
    "    year = None\n",
    "    \n",
    "    if l == 3:\n",
    "        month = int(segments[0])\n",
    "        day = int(segments[1])\n",
    "        year = int(segments[2])\n",
    "    \n",
    "    if l == 2:\n",
    "        month = int(segments[0])\n",
    "        year = int(segments[1])\n",
    "    \n",
    "    if l == 1:\n",
    "        year = int(segments[0])\n",
    "        \n",
    "    return (year, month, day)\n",
    "        \n",
    "\n",
    "# w_slash = single_dates[single_dates['CompletionDateRaw'].str.contains('/', na=False)]\n",
    "\n",
    "# w_slash['CompletionDate'] = w_slash['CompletionDateRaw'].apply(parse_slash_date)\n",
    "\n",
    "\n",
    "def parse_word_month(string):\n",
    "    \n",
    "    months = {\n",
    "        'jan':1,\n",
    "        'january':1,\n",
    "        'feb':2,\n",
    "        'february':2,\n",
    "        'mar':3,\n",
    "        'march':3,\n",
    "        'apr':4,\n",
    "        'april':4,\n",
    "        'may':5,\n",
    "        'jun':6,\n",
    "        'june':6,\n",
    "        'jul':7,\n",
    "        'july':7,\n",
    "        'aug':8,\n",
    "        'august':8,\n",
    "        'sep':9,\n",
    "        'sept':9,\n",
    "        'september':9,\n",
    "        'oct':10,\n",
    "        'october':10,\n",
    "        'nov':11,\n",
    "        'november':11,\n",
    "        'dec':12,\n",
    "        'december':12,\n",
    "        'spring':6,\n",
    "        'fall':12,\n",
    "        'summer':9,\n",
    "        'winter':1\n",
    "    }\n",
    "    \n",
    "    if string is pd.NA:\n",
    "        return pd.NA\n",
    "    \n",
    "    string = string.replace(',', ' ')\n",
    "    string = string.replace('.', ' ')\n",
    "    string = string.replace('\\'', ' ')\n",
    "    string = string.replace('-', ' ')\n",
    "    string = string.replace('/', ' ')\n",
    "    string = string.replace('1st', ' 1 ')\n",
    "    string = string.replace('nd', ' ')\n",
    "    string = string.replace('rd', ' ')\n",
    "    string = string.replace('th', ' ')\n",
    "    string = string.replace('?', '')\n",
    "    string = string.replace('of', ' ')\n",
    "    \n",
    "    pieces = string.split(' ')\n",
    "    \n",
    "    no_empty = list()\n",
    "    for p in pieces:\n",
    "        if len(p) > 0:\n",
    "            no_empty.append(p)\n",
    "    \n",
    "    pieces = no_empty\n",
    "    \n",
    "    year = None\n",
    "    month = None\n",
    "    day = None\n",
    "    \n",
    "    try:\n",
    "        if len(pieces) == 2:\n",
    "            month = pieces[0]\n",
    "            day = 1\n",
    "            year = int(pieces[1])\n",
    "\n",
    "        elif len(pieces) == 3:\n",
    "            month = pieces[0]\n",
    "            day = int(pieces[1])\n",
    "            year = int(pieces[2])\n",
    "\n",
    "        elif len(pieces) == 1:\n",
    "            year = int(pieces[0])\n",
    "            month = 'june'\n",
    "            day = 1\n",
    "\n",
    "        else:\n",
    "            return pd.NA\n",
    "        \n",
    "    except(ValueError):\n",
    "        if len(pieces) == 1:\n",
    "            print(string)\n",
    "            for k in months.keys():\n",
    "                if string.find(k) >=0:\n",
    "                    month = k\n",
    "                    break\n",
    "            remainder = string.replace(k, '')\n",
    "            try:\n",
    "                year = int(remainder)\n",
    "            except(ValueError):\n",
    "                return pd.NA\n",
    "            \n",
    "            if month is None:        \n",
    "                return pd.NA\n",
    "        else:\n",
    "            return pd.NA\n",
    "        \n",
    "    if month in months.keys():\n",
    "        month = months[month]\n",
    "    else:\n",
    "        try: \n",
    "            month = int(month)\n",
    "        except(ValueError):\n",
    "            return pd.NA\n",
    "        \n",
    "    \n",
    "    return (year, month, day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def parse_date(string):\n",
    "    \n",
    "    string = string.lower()\n",
    "    \n",
    "    if string.find('/') >= 0:\n",
    "        date = parse_slash_date(string)\n",
    "        if date is pd.NA:\n",
    "            date = parse_word_month(string)\n",
    "    \n",
    "    else:\n",
    "        date = parse_word_month(string)\n",
    "    \n",
    "    if date is pd.NA:\n",
    "        return pd.NA\n",
    "    \n",
    "    year = date[0]\n",
    "    month = date[1]\n",
    "    day = date[2]\n",
    "    \n",
    "    if (year is None) & (month is None) & (day is None):\n",
    "        return pd.NA\n",
    "    \n",
    "    if year is None:\n",
    "        return pd.NA\n",
    "    \n",
    "    year = abs(year)\n",
    "    \n",
    "    if month > 1926:\n",
    "        temp = month\n",
    "        month = year\n",
    "        year = temp\n",
    "        del(temp)  \n",
    "    \n",
    "    if year < 100:\n",
    "        y = str(year)\n",
    "        if year < 26:\n",
    "            year = int('20' + f\"{y:0>2}\")\n",
    "        else:\n",
    "            year = int('19' + f\"{y:0>2}\")\n",
    "        \n",
    "    if (month > 12) & (month < 31):\n",
    "        temp = day\n",
    "        day = month\n",
    "        month = temp\n",
    "        del(temp)\n",
    "    \n",
    "    if day is None: \n",
    "        day = 1\n",
    "    \n",
    "    if month is None:\n",
    "        month = 6\n",
    "    \n",
    "    try:\n",
    "        date = dt.date(year, month, day)\n",
    "        return date\n",
    "    except(ValueError):\n",
    "#         print(string, '->', month, day, year)\n",
    "        return pd.NA\n",
    "    \n",
    "single_dates['CompletionDate'] = single_dates['CompletionDateProc'].apply(parse_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_dates[single_dates['CompletionDate'].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_dates[~single_dates['CompletionDate'].isnull()&(single_dates['ind_doc']==1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = jobs.merge(single_dates[single_dates['ind_doc']==1][['BGTResID', 'CompletionDate']], how='inner')\n",
    "\n",
    "temp['jobISOStartDate'] = pd.to_datetime(temp['jobISOStartDate'], errors='coerce')\n",
    "temp['jobISOEndDate'] = pd.to_datetime(temp['jobISOEndDate'], errors='coerce')\n",
    "temp['CompletionDate'] = pd.to_datetime(temp['CompletionDate'], errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(temp.info())\n",
    "\n",
    "greater = temp[temp['jobISOStartDate']>temp['CompletionDate']]\n",
    "\n",
    "display(greater)\n",
    "\n",
    "greater['BGTResID'].nunique()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp.groupby(['BGTResID', 'CompletionDate', 'jobISOStartDate']).count()[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "months = {\n",
    "        'jan':1,\n",
    "        'january':1,\n",
    "        'feb':2,\n",
    "        'february':2,\n",
    "        'mar':3,\n",
    "        'march':3,\n",
    "        'apr':4,\n",
    "        'april':4,\n",
    "        'may':5,\n",
    "        'jun':6,\n",
    "        'june':6,\n",
    "        'jul':7,\n",
    "        'july':7,\n",
    "        'aug':8,\n",
    "        'august':8,\n",
    "        'sep':9,\n",
    "        'september':9,\n",
    "        'oct':10,\n",
    "        'october':10,\n",
    "        'nov':11,\n",
    "        'november':11,\n",
    "        'dec':12,\n",
    "        'december':12\n",
    "}\n",
    "\n",
    "print(months.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_dates.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "container = set()\n",
    "\n",
    "def find_non_numbers(string):\n",
    "    if string is not pd.NA:\n",
    "        for s in string.split(' '):\n",
    "            try:\n",
    "                int(s)\n",
    "            except(ValueError):\n",
    "                container.add(s)\n",
    "                \n",
    "single_dates['CompletionDateRaw'].apply(find_non_numbers)\n",
    "\n",
    "container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for s in container:\n",
    "    if s.find('/') >= 0:\n",
    "        print(s.replace('?', ''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_unicode_escapes(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_s = r'{}'.format(string)\n",
    "\n",
    "print(raw_s.find('\\\\'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output[(output['MajorCipCode_pound']==0)&(output['major_pound']==0)&(output['degreeLevel_pound']==0)&(output['CompletionDateRaw_pound']==0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_doc[final_doc['CompletionDateRaw_pound']>=30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Doctoral indicator dummy generation code\n",
    "\n",
    "def define_doctoral(df):\n",
    "\n",
    "    _21 = df[df['degreeLevel'].str.contains('21', na=False)]\n",
    "    \n",
    "    print('\\tNumber of \\'21\\'s found:', _21['BGTResID'].nunique())\n",
    "        \n",
    "    doc = df.drop(df.index.difference(_21.index))\n",
    "    wo_21 = df.drop(_21.index)\n",
    "\n",
    "    strings = ['Doctor', 'ph\\.', 'm\\.d\\.', 'j\\.d\\.', 'phd', 'dds', 'dml', 'ed\\. D']\n",
    "\n",
    "    for string in strings:\n",
    "        target = wo_21[wo_21['DegreeType'].str.contains(string, case=False, na=False)]\n",
    "\n",
    "        print('\\tContains \\'' + string + '\\':', target['BGTResID'].nunique())\n",
    "\n",
    "        wo_21 = wo_21.drop(target.index)\n",
    "        doc = pd.concat([doc, target])\n",
    "        \n",
    "    \n",
    "    return doc\n",
    "\n",
    "doc = define_doctoral(education)\n",
    "\n",
    "doc['ind_doc'] = 1\n",
    "education['ind_doc'] = 0\n",
    "\n",
    "doc = pd.concat([doc, education.loc[education.index.difference(doc.index)]], sort=False)\n",
    "\n",
    "doc.to_csv(r'A:\\_DataBGTRes\\Doctoral_Data\\doc_education_info_with_indicator.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmarks of CIP data\n",
    "\n",
    "def print_benchmarks(df):\n",
    "    total = df['BGTResID'].nunique()\n",
    "\n",
    "    full_both = df[~df['MajorCipCode'].isnull()&~df['major'].isnull()]['BGTResID'].nunique()\n",
    "\n",
    "    no_CIP = df[df['MajorCipCode'].isnull()&~df['major'].isnull()]['BGTResID'].nunique()\n",
    "\n",
    "    null_both = df[df['MajorCipCode'].isnull()&df['major'].isnull()]['BGTResID'].nunique()\n",
    "\n",
    "    print(total, '\\n\\t' + str(full_both) + ' -> ' + str(round((float(full_both)/float(total)) * 100, 2)) + '%',\n",
    "         '\\n\\t' + str(no_CIP) + ' -> ' + str(round((float(no_CIP)/float(total)) * 100, 2)) + '%',\n",
    "         '\\n\\t' + str(null_both) + ' -> ' + str(round((float(null_both)/float(total)) * 100, 2)) + '%')\n",
    "    \n",
    "print_benchmarks(education[education['ind_doc']==1])\n",
    "print_benchmarks(education[education['ind_doc']==0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = education[education['ind_doc']==1]\n",
    "len(df)/df['BGTResID'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = education[education['ind_doc']==0]\n",
    "len(df)/df['BGTResID'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(doc)/doc['BGTResID'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "education[~education['MajorCipCode'].isnull()&~education['major'].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_cip = pd.DataFrame(education[(education['MajorCipCode'].isnull())]['major'].value_counts())\n",
    "\n",
    "print(len(no_cip))\n",
    "display(no_cip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(education[education['major']=='Biology']['DegreeType'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cips = pd.read_csv(r'https://nces.ed.gov/ipeds/cipcode/Files/CIPCode2010.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "cips[cips['CIPTitle'].str.contains('Biology', case=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ELIAS CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Set data locations\n",
    "input_loc = '[SET TO INTERMEDIATE DATA LOCATION]'\n",
    "output_loc = '[SET TO DESIRED OUTPUT LOCATION]'\n",
    "onet_url = 'https://www.onetcenter.org/taxonomy/2010/soc2018/2010_to_2018_SOC_Crosswalk.csv?fmt=csv'\n",
    "naics_url = 'https://www.census.gov/eos/www/naics/2017NAICS/2-6%20digit_2017_Codes.xlsx'\n",
    "cip_url = 'https://nces.ed.gov/ipeds/cipcode/Files/CIPCode2010.csv'\n",
    "\n",
    "onet = pd.read_csv(onet_url)\n",
    "onet.rename(columns = {\n",
    "    'O*NET-SOC 2010 Code': 'ONETCode', \n",
    "    'O*NET-SOC 2010 Title': 'ONETName', \n",
    "    '2018 SOC Code': 'SOCCode',\n",
    "    '2018 SOC Title': 'SOCName'\n",
    "    }, inplace = True)\n",
    "naics = pd.read_excel(naics_url)\n",
    "naics.rename(columns = {\n",
    "    '2017 NAICS US   Code': 'NAICS2',\n",
    "    '2017 NAICS US Title': 'NAICSName'\n",
    "    }, inplace = True)\n",
    "naics = naics[['NAICS2', 'NAICSName']]\n",
    "\n",
    "jobs = pd.read_csv(f'{input_loc}04_PhD_Jobs.csv')\n",
    "\n",
    "jobs = jobs.merge(onet, on = 'ONETCode')\n",
    "jobs = jobs.merge(naics, on = 'NAICS2')\n",
    "jobs = jobs[[\n",
    "    'BGTResID', 'StartDate', 'EndDate', \n",
    "    'ONETCode', 'ONETName', 'SOCCode', 'SOCName',\n",
    "    'NAICS2', 'NAICSName'\n",
    "    ]]\n",
    "\n",
    "cip = pd.read_csv(cip_url)\n",
    "cip = cip[['CIPCode', 'CIPTitle']]\n",
    "cip['CIPCode'] = cip['CIPCode'].str.replace('=', '', regex = False)\n",
    "cip['CIPCode'] = cip['CIPCode'].str.replace('\"', '', regex = False)\n",
    "\n",
    "phds = pd.read_csv(f'{input_loc}03_PhD_CIP_codes.csv', index_col = 'BGTResID')\n",
    "phds['PhD_CIPs'] = phds['PhD_CIPs'].str.replace(';', '#', regex = False)\n",
    "phds['PhD_CIPs'] = phds['PhD_CIPs'].str.replace(' ', '#', regex = False)\n",
    "for string in ['38.0001', '38.0101', '38.0199', '38.9999']:\n",
    "    pat = string + '#'\n",
    "    phds['PhD_CIPs'] = phds['PhD_CIPs'].str.replace(pat, '', regex = False)\n",
    "    pat = '#' + string\n",
    "    phds['PhD_CIPs'] = phds['PhD_CIPs'].str.replace(pat, '', regex = False)\n",
    "\n",
    "max_splits = phds['PhD_CIPs'].str.count('#').max()\n",
    "print(f'The most Ph.D. CIPs associated with a resume is {max_splits + 1}')\n",
    "phds_split = phds['PhD_CIPs'].str.split(pat = '#', expand = True\n",
    "    ).fillna(value = '')\n",
    "phds_split = phds_split.reset_index()\n",
    "cip.rename(columns = {\n",
    "    'CIPCode': 'CIPCode0',\n",
    "    'CIPTitle': 'CIPName0',\n",
    "    }, inplace = True)\n",
    "columns = ['BGTResID']\n",
    "for code in range(max_splits.astype(int) + 1):\n",
    "    phds_split.rename(columns = {\n",
    "        code: f'CIPCode{code}',\n",
    "        }, inplace = True)\n",
    "    assert phds_split[f'CIPCode{code}'].str.len().max() <= 7\n",
    "    phds_split = phds_split.merge(cip, how = 'left', on = f'CIPCode{code}')\n",
    "    cip.rename(columns = {\n",
    "        f'CIPCode{code}': f'CIPCode{code + 1}',\n",
    "        f'CIPName{code}': f'CIPName{code + 1}'\n",
    "        }, inplace = True)\n",
    "    columns.append(f'CIPCode{code}')\n",
    "    columns.append(f'CIPName{code}')\n",
    "phds_split = phds_split[columns]\n",
    "\n",
    "# phds_split.to_csv(f'{output_loc}PhD_CIP_codes.csv', index = False)\n",
    "# jobs.to_csv(f'{output_loc}PhD_Jobs.csv', index = False)\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
