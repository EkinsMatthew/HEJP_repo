{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('fivethirtyeight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_directory = r\"C:\\Users\\Public\\_Data\\Old_HEJP_Data\\Latest_Version\"\n",
    "\n",
    "jobs_table = pd.read_csv(root_directory + r'\\Main_Data\\Main_Table_01192020.csv')\n",
    "print(len(main_table))\n",
    "nsf_table = pd.read_csv(root_directory + r'\\Faculty_Data\\Faculty_Table_11222019.csv')\n",
    "print(len(faculty_table))\n",
    "skill_table = pd.read_csv(root_directory + r'\\Skills_Data\\Skill_Table_06072019.csv')\n",
    "print(len(skill_table))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some Career Areas and Occupations need to have their names clarified with this function. You will see it\n",
    "# applied with: df[column_name] = df[column_name].apply(title_swap). This is needed since occasionaly the\n",
    "# BG name for the occupational category does not describe it clearly in the context of Higher Education.\n",
    "# Take for example the category of 'Community and Social Services'. It is not self evident what would exist\n",
    "# as a job in that category. Are the social services unemployment or homeless services? No, but that is the\n",
    "# connotation 'Social Services' carries. Therefore, we change the outward facing title to be 'Counseling\n",
    "# and Religious Life'.\n",
    "def title_swap(string):\n",
    "    dictionary = {'Community and Social Services':'Counseling and Religious Life',\n",
    "                  'Customer and Client Support':'Online Support and University Information',\n",
    "                  'Hospitality, Food, and Tourism':'Event Management and Hospitality', \n",
    "                  'Planning and Analysis':'Analysis', \n",
    "                  'Curriculum and Instructional Designer / Developer':'Curriculum and Instructional Designer', \n",
    "                  'Special Education Teacher':'Accessibility and Disability Services', \n",
    "                  'Teaching Assistant':'Faculty Support', \n",
    "                  'Tutor':'Academic Tutor'}\n",
    "    if(string in dictionary):\n",
    "        return dictionary[string]\n",
    "    else:\n",
    "        return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#####################################################\n",
    "# Visual 1\n",
    "# Number of Occupations and Career Areas \"requesting\"\n",
    "# a skill in 2010 and 2017 w/ Added and Dropped\n",
    "#####################################################\n",
    "\n",
    "# Skill Cluster to run diffusion anaysis on\n",
    "target_skill = 'Mental and Behavioral Health Specialties'\n",
    "\n",
    "# Category to break the dataset down by (can be Career Area or Occupation)\n",
    "category = 'Career Area'\n",
    "\n",
    "# Remove null in the category of interest\n",
    "jobs = jobs_table[jobs_table[category]!='na']\n",
    "\n",
    "# Chosen years\n",
    "year1 = 2010\n",
    "year2 = 2017\n",
    "\n",
    "# Prevent user input error which would give garbled results\n",
    "if year2 <= year1:\n",
    "    raise ValueError(f'Year 2: ({year2}) must be greater than Year 1: ({year1})')\n",
    "    \n",
    "# Table for fining all categorical counts in certain years\n",
    "dataframe = jobs[(jobs['Year']==year1)|(jobs['Year']==year2)][['Job ID', 'Year', category]]\n",
    "\n",
    "# Skills Dataframe restricted to the target skill and years\n",
    "skill_df = skill_table[(skill_table['Year']==year1)|(skill_table['Year']==year2)]\n",
    "\n",
    "# Merge in the category of intrest to make finding the number with a skill easier\n",
    "skill_df = skill_df[skill_df['Skill Cluster Name']==target_skill].merge(jobs[['Job ID', category]], how='inner')\n",
    "\n",
    "# Since we are measuring the requests of Skill Clusters and not Skill Names, it could be\n",
    "# the case that a particular posting requests two skills from the same cluster. By the\n",
    "# method below, that job would then be counted multiple times in the same category.\n",
    "# Therefore, duplicates must be dropped on Job ID\n",
    "\n",
    "skill_df = skill_df.drop_duplicates('Job ID')\n",
    "\n",
    "# Among the most valuable analyses that the HEJP dataset makes possible is of the \n",
    "# distinct skills required by specific jobs. This enables tracking of emerging \n",
    "# skill-needs in specific jobs, as well as many other related skill-based analytics.\n",
    "# To do any of these analytics requires us to define minimum thresholds of occurrence.\n",
    "# Otherwise, the resulting claims about stand-out skills will become non-sensical as the \n",
    "# analytics cannot help but emphasize dramatic growth based on unreliable baselines of a \n",
    "# tiny number of occurrences.\n",
    "\n",
    "# We chose 5% as the minimal number of occurrences, or requests, of a specific skill within \n",
    "# the particular occupation or career area we aim to study. We use percentage occurrence \n",
    "# rather than absolute number of the particular skill within the \"bucket,\" or \n",
    "# occupation/career area we wish to study, because percentages allow for varying sizes of \n",
    "# buckets.\n",
    "\n",
    "# The choice of 5% as a threshold begs the question: if we have an occupation, or bucket,\n",
    "# in which there are only 2 postings, does one of them requesting a skill count as the \n",
    "# occupation as a whole requesting it? What about in the case of 10 postings with only \n",
    "# one requesting? In that case, 10% of the sample requests the skill; is that enough to \n",
    "# get above our threshold? \n",
    "\n",
    "# The answer to these questions is simply: we can't be sure. We need to utilize a \n",
    "# statistical test in order to be confident that the bucket, or category, in question \n",
    "# 'requests' the skill. What we are looking to estimate is the true proportion of jobs \n",
    "# that requests a given skill in the bucket. Since a job either has or does not have that\n",
    "# skill requested in its description it can be described as a binary variable = 1 if the \n",
    "# posting requests that skill and = 0 if it does not. More specifically we can treat this \n",
    "# new dummy as a Bernouli random variable where p is the true proportion of jobs that \n",
    "# request a skill, 1-p is the proportion that do not ask for that skill. A sample mean can \n",
    "# be obtained from this value and then it can be tested to see if it is statistically \n",
    "# different from zero (the null hypothesis).\n",
    "\n",
    "# In our hypothetical test, we want to know the minimum n for which 5% of that sample = 1\n",
    "# is statistically different from 0. That n will provide us a cutoff_threshold for which\n",
    "# we can require cateogires meet in order to be considered for the 5% test. Above that n \n",
    "# we know that it will not have to be tested again since it will only ever become more \n",
    "# significant as n increases but p stays at 5%.\n",
    "\n",
    "# For this calculation we need sample(p) = % with skill, sample_variance = p*(1-p) \n",
    "# [Variance of a Bernouli Random Variable], SE(p) = sqrt(var(p)/n), and t = (p - 0)/SE(p). \n",
    "# We can then solve for an n that gives us t = 1.64 and pr(Null Hypothesis) = 0.05.\n",
    "\n",
    "# 1.64 = 0.05/sqrt((0.05*0.95)/n) -> 0.05/1.64 = sqrt((0.0475)/n) -> \n",
    "# (0.030488)^2 = (0.0475)/n -> n = 0.0475/(0.030488)^2 -> n = 51.1017 or 50\n",
    "\n",
    "# This is the threshold that we set in order to ensure we can trust the 5% value coming out\n",
    "# of a set of postings in an occupation. Any Occupation with fewer than 50 observations in \n",
    "# either year will not be counted since the results of the 5% proportion test will not be \n",
    "# valid. Of course we could simply run a t-test on each row individually. However, we are \n",
    "# fairly sure that this dataset does not represent a random sample. Therefore, our methods\n",
    "# are guided by statistical tests even if they cannot be used confidently.\n",
    "\n",
    "def count_with_skill(df, skill_df, year, drop=True, cutoff_threshold=50, minimum_percent_occurrence=0.05):\n",
    "    \n",
    "    # Generate ordered list of the occupations in both years. These show how many obervations within each\n",
    "    # Occupation request the skill of Fundraising\n",
    "    occ_skill = pd.DataFrame(skill_df[skill_df['Year']==year][category].value_counts()).reset_index()\n",
    "    occ_skill = occ_skill.rename(columns={category : '#skill', 'index' : category})\n",
    "#     display(occ_skill)\n",
    "    \n",
    "    occ_df = pd.DataFrame(df[df['Year']==year][category].value_counts().reset_index())\n",
    "    occ_df = occ_df.rename(columns={category : 'count', 'index' : category})\n",
    "#     display(occ_df)\n",
    "    \n",
    "    occ = occ_df.merge(occ_skill, how='inner', on=category)\n",
    "    \n",
    "    if drop:\n",
    "        occ = occ[occ['count']>=cutoff_threshold]\n",
    "    \n",
    "    occ['prop'] = np.true_divide(occ['#skill'], occ['count'])\n",
    "    occ = occ.sort_values(by='prop', ascending=False)\n",
    "    \n",
    "    if drop:\n",
    "        occ = occ[occ['prop']>=minimum_percent_occurrence]\n",
    "    \n",
    "    occ[category] = occ[category].apply(title_swap)\n",
    "    \n",
    "    return occ \n",
    "\n",
    "# The dataframe of the first year with raw counts and percent occurrence\n",
    "counts_1 = count_with_skill(dataframe, skill_df, year1)\n",
    "\n",
    "# The dataframe of the second year with the same as above\n",
    "counts_2 = count_with_skill(dataframe, skill_df, year2)\n",
    "\n",
    "# Perform outer merge on the to dataframes to identify how the job space has changed between\n",
    "# the sample years\n",
    "full = counts_1[[category, 'prop']].merge(counts_2[[category, 'prop']], on=category, how='outer', indicator=True)\n",
    "\n",
    "# Utilize indicator from outer merge to identify which categories are newly above the \n",
    "# minimum threshold\n",
    "gained = full[full['_merge']=='right_only']\n",
    "gained = gained.drop(columns=['prop_x', '_merge'])\n",
    "gained = gained.rename(columns={'prop_y':'prop'})\n",
    "\n",
    "# and which categories have fallen below in the time period\n",
    "lost = full[full['_merge']=='left_only']\n",
    "lost = lost.drop(columns=['prop_y', '_merge'])\n",
    "lost = lost.rename(columns={'prop_x':'prop'})\n",
    "\n",
    "# Display all printed output that describes the number of total occupations in both years with the\n",
    "# target skill, as well as how many came obove the minimum threshold and dropped below it.\n",
    "print('Skill Analysis: [' + target_skill + ']\\n')\n",
    "print('#', category, str(year1), ':', len(counts_1))\n",
    "print('#', category, str(year2), ':', len(counts_2))\n",
    "print(year1, '->', year2, '\\n\\tNumber of ' + category + 's Added:', len(gained),\n",
    "      '\\n\\tNumber of ' + category + 's Dropped:', len(lost), '\\n')\n",
    "\n",
    "# Show which of the categories existed in the first year\n",
    "print(f'Top {category}s requesting \"{target_skill}\" in {year1}:')\n",
    "display(counts_1[[category]][:10])\n",
    "\n",
    "# And which were added in the second year\n",
    "print(f'Top {category}s added in {year2}')\n",
    "display(gained[[category]][:10])\n",
    "\n",
    "def percent_occurrence_graph(df, category, target_skill, title):\n",
    "    df = df.sort_values(by='prop', ascending = False)\n",
    "    \n",
    "    rates = df['prop'][:10].values\n",
    "    names = df[category][:10].values\n",
    "    \n",
    "    ind = np.array([x for x, _ in enumerate(names)])\n",
    "    \n",
    "    plt.bar(ind, rates)\n",
    "    \n",
    "    plt.xticks(ind, names, rotation = 'vertical')\n",
    "    plt.xlabel(category + 's')\n",
    "    plt.ylabel(f'Proporion of {category} requesting\\n \"{target_skill}\" Skills')\n",
    "    plt.title(title)\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "# Display the percent occurrence of the top 10 of the target category in the first year\n",
    "percent_occurrence_graph(counts_1, category, target_skill, 'Top ' + category + 's in ' + str(year1))\n",
    "\n",
    "# Display for the second year\n",
    "percent_occurrence_graph(counts_2, category, target_skill, 'Top ' + category + 's in ' + str(year2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################\n",
    "# Visual 2\n",
    "# Change in Percentage of Jobs Requesting a Skill\n",
    "# by Career Area or Occupation 2010-2017\n",
    "#################################################\n",
    "\n",
    "# Chose to not drop so that we can have a defined change between\n",
    "# the added categories and their antecedent in 2010\n",
    "counts_1_unfiltered = count_with_skill(dataframe, skill_df, 2010, drop=False)\n",
    "\n",
    "# Merge with counts from 2017 in last visual\n",
    "total = counts_1_unfiltered.merge(counts_2, on=category, how='inner')\n",
    "\n",
    "# Change is just the raw difference between the categories occurrence\n",
    "total['change'] = total['prop_y'] - total['prop_x']\n",
    "\n",
    "total = total.sort_values('change', ascending=False)\n",
    "\n",
    "# Graph using pyplot\n",
    "def display_change(df, category, target_skill, title, top=10):\n",
    "    \n",
    "    rates = df['change'][:top].values\n",
    "    names = df[category][:top].values\n",
    "    ind = np.arange(len(rates))\n",
    "    \n",
    "    plt.bar(ind, rates, label='Change in Percentage')\n",
    "    \n",
    "    plt.xticks(ind, names, rotation='vertical')\n",
    "    plt.xlabel(category + 's')\n",
    "    plt.ylabel('Change in Percentage\\nOccurrence')\n",
    "    plt.title(title)\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "display_change(total, category, target_skill, f'Change in Occurrence of \"{target_skill}\"\\nin top 2017 {category}s from 2010')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
