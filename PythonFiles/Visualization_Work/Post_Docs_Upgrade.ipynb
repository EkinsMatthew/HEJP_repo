{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', 50)\n",
    "pd.set_option('display.max_rows', 200)\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# import probscale\n",
    "import scipy\n",
    "import scipy.stats as stats\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feel free to use any of the matplot styles that fit your fancy\n",
    "# link to the doc: https://matplotlib.org/gallery/style_sheets/style_sheets_reference.html\n",
    "plt.style.use('fivethirtyeight')\n",
    "\n",
    "# Needed Until reformat of BGT Upgrade is complete\n",
    "def format_df(df):\n",
    "    print(len(df))\n",
    "    df = df.rename(columns={'BGTJobId':'Job ID'})\n",
    "    return df\n",
    "\n",
    "# Import relevant dataframes:\n",
    "main_table = pd.read_parquet(r\"A:\\HEJP_revision_02152020\\Jobs.parquet\")\n",
    "# print(len(main_table))\n",
    "main_table = format_df(main_table)\n",
    "\n",
    "faculty_table = pd.read_parquet(r\"A:\\HEJP_revision_02152020\\Faculty_and_Postdoc_Fields.parquet\")\n",
    "# print(len(faculty_table))\n",
    "faculty_table = format_df(faculty_table)\n",
    "\n",
    "skill_table = pd.read_parquet(r\"A:\\HEJP_revision_02152020\\Skills.parquet\")\n",
    "# print(len(skill_table))\n",
    "skill_table = format_df(skill_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BEA_zone(state):\n",
    "    \n",
    "    dictionary = {'KS': 'Plains',\n",
    "                  'MS': 'Southeast',\n",
    "                  'NM': 'Southwest',\n",
    "                  'MN': 'Plains',\n",
    "                  'GA': 'Southeast',\n",
    "                  'TN': 'Southeast',\n",
    "                  'PA': 'Mideast',\n",
    "                  'OH': 'Great Lakes',\n",
    "                  'WI': 'Great Lakes',\n",
    "                  'NJ': 'Mideast',\n",
    "                  'TX': 'Southwest',\n",
    "                  'AZ': 'Southwest',\n",
    "                  'CA': 'Far West',\n",
    "                  'MA': 'New England',\n",
    "                  'FL': 'Southeast',\n",
    "                  'RI': 'New England',\n",
    "                  'NC': 'Southeast',\n",
    "                  'IL': 'Great Lakes',\n",
    "                  'KY': 'Southeast',\n",
    "                  'NV': 'Far West',\n",
    "                  'CO': 'Rocky Mountains',\n",
    "                  'DC': 'Mideast',\n",
    "                  'VA': 'Southeast',\n",
    "                  'IA': 'Plains',\n",
    "                  'UT': 'Rocky Mountains',\n",
    "                  'IN': 'Great Lakes',\n",
    "                  'NH': 'New England',\n",
    "                  'OR': 'Far West',\n",
    "                  'MO': 'Plains',\n",
    "                  'DE': 'Mideast',\n",
    "                  'CT': 'New England',\n",
    "                  'MI': 'Great Lakes',\n",
    "                  'SC': 'Southeast',\n",
    "                  'MT': 'Rocky Mountains',\n",
    "                  'OK': 'Southwest',\n",
    "                  'NY': 'Mideast',\n",
    "                  'ID': 'Rocky Mountains',\n",
    "                  'WV': 'Southeast',\n",
    "                  'MD': 'Mideast',\n",
    "                  'AK': 'Far West',\n",
    "                  'AR': 'Southeast',\n",
    "                  'NE': 'Plains',\n",
    "                  'AL': 'Southeast',\n",
    "                  'LA': 'Southeast',\n",
    "                  'WA': 'Far West',\n",
    "                  'HI': 'Far West',\n",
    "                  'VT': 'New England',\n",
    "                  'ME': 'New England',\n",
    "                  'SD': 'Plains',\n",
    "                  'WY': 'Rocky Mountains',\n",
    "                  'ND': 'Plains',\n",
    "                  'PR': 'Territories',\n",
    "                  'GU': 'Territories',\n",
    "                  'VI': 'Territories',\n",
    "                  'MP': 'Territories',\n",
    "                  'AS': 'Territories',\n",
    "                  'MH': 'Territories',\n",
    "                  'FM': 'Territories'}\n",
    "    \n",
    "    if type(state) is str and state == 'na':\n",
    "        return None\n",
    "    elif type(state) is str:\n",
    "        return dictionary[state]\n",
    "    elif state is None:\n",
    "        return None\n",
    "    \n",
    "main_table['BEA_zone']=main_table['State'].apply(BEA_zone)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################################\n",
    "# Generating Institution based State and BEA_zone identifiers\n",
    "#############################################################\n",
    "# 01/19/2020\n",
    "\n",
    "main = main_table.copy()\n",
    "\n",
    "# Get all states that each institution is found in and the number of postings in those states\n",
    "states = pd.DataFrame(main_table.groupby(['IPEDS Institution Name', 'State']).count()['Job ID'])\n",
    "\n",
    "# Get ['IPEDS Institution Name', 'State'] as workable columns\n",
    "states = states.reset_index()\n",
    "\n",
    "# Resort the table lexicographically by institution name, and descending by number of postings within\n",
    "states = states.sort_values(by=['IPEDS Institution Name', 'Job ID'], ascending=[True, False])\n",
    "\n",
    "# Drop duplicates on institution s.t. only the top counted state is kept\n",
    "states = states.drop_duplicates('IPEDS Institution Name')\n",
    "\n",
    "# Dictionary for mapping institution to its most common state\n",
    "mapping = {}\n",
    "\n",
    "def populate_mapping(row, mapping):\n",
    "    '''\n",
    "    Method for mapping each Institution with its primary state. Meant to be caled with the\n",
    "    Pandas.DataFrame.apply() function.\n",
    "    '''\n",
    "    \n",
    "    mapping[row['IPEDS Institution Name']] = row['State']\n",
    "    \n",
    "# Get mapping    \n",
    "states.apply(populate_mapping, axis=1, args=(mapping,))\n",
    "\n",
    "# Function returns the primary state of the institution\n",
    "def primary_state(inst):\n",
    "    if type(inst) is str and inst == 'na':\n",
    "        return None\n",
    "    elif type(inst) is str:\n",
    "        try:\n",
    "            return mapping[inst]\n",
    "        except KeyError:\n",
    "            return None\n",
    "    elif inst is None:\n",
    "        return None\n",
    "        \n",
    "# Generate new 'Institution State' column for main table and BEA_zone from that\n",
    "main['Institution_State'] = main['IPEDS Institution Name'].apply(primary_state)\n",
    "main['Institution_BEA_zone'] = main['Institution_State'].apply(BEA_zone)\n",
    "\n",
    "# Fix previous BEA_zone column\n",
    "main['BEA_zone'] = main['State'].apply(BEA_zone)\n",
    "\n",
    "main_table = main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del(main)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Series of post-doc IDs useful for isolating all ['Post-Doctoral' = 1] postings by Job ID\n",
    "mask = main_table[main_table['Postdoc']==1][['Job ID']]\n",
    "print(len(mask))\n",
    "# This can then be used to perform a natural join on any of the above tables and extract\n",
    "# post-doc only data. It is far simpler than trying to isolate the post-doc data using the\n",
    "# faculty table each time; it saves memory and time as well!\n",
    "\n",
    "############################################################\n",
    "# General Functions\n",
    "############################################################\n",
    "\n",
    "def breakout(df, year, category):\n",
    "    '''\n",
    "    Generalized breakout for obtaining sizes of categories within a data column.\n",
    "    \n",
    "    Parameters:\n",
    "    df (DataFrame): Full Pandas DataFrame used as the source for the breakout\n",
    "    year (int): The year to which df will be restricted\n",
    "    category (string): The cateogry by which the user would like to break out\n",
    "    \n",
    "    Returns:\n",
    "    DataFrame with \n",
    "    '''\n",
    "    num = df['Job ID'].nunique()\n",
    "    df = df[df['Year']==year]\n",
    "    cat = pd.DataFrame(df[category].value_counts()).reset_index()\n",
    "    cat = cat.rename(columns={category:'count'})\n",
    "    cat['inc'] = np.true_divide(cat['count'], num)\n",
    "    \n",
    "    return cat.sort_values(by='count', ascending=False)\n",
    "\n",
    "def growth(df1, df2):\n",
    "    df = df1.merge(df2, on='index', how='outer', indicator=True)\n",
    "    df = df[df['_merge']=='both']\n",
    "#     df['growth'] = np.true_divide(df['inc_y'] - df['inc_x'], df['inc_x'])\n",
    "    df['growth'] = np.true_divide(df['count_y'] - df['count_x'], df['count_x'])\n",
    "    return df.sort_values('growth', ascending=False).reset_index(drop=True)\n",
    "    \n",
    "############################################################\n",
    "# Visualization 1)\n",
    "# Post-Doc vs HEJP baseline by BEA Zone\n",
    "############################################################    \n",
    "\n",
    "# Use mask to obtain main_table subset that contains strictly post-doc postings\n",
    "post = main_table.merge(mask, on='Job ID', how='inner')\n",
    "post = post[~post['IPEDS Institution Name'].isnull()]\n",
    "\n",
    "# Store the complement of this subset (all non-post-doc postings)\n",
    "main = main_table.drop(post.index)\n",
    "main = main[main['BEA_zone']!='Territories']\n",
    "\n",
    "year1 = 2007\n",
    "year2 = 2019\n",
    "\n",
    "main_zone1 = breakout(main, year1, 'Institution_BEA_zone')\n",
    "main_zone2 = breakout(main, year2, 'Institution_BEA_zone')\n",
    "\n",
    "main_zone_g = growth(main_zone1, main_zone2)\n",
    "\n",
    "zone1 = breakout(post, year1, 'Institution_BEA_zone')\n",
    "zone2 = breakout(post, year2, 'Institution_BEA_zone')\n",
    "\n",
    "zone_g = growth(zone1, zone2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Superimposed Categorical growth:\n",
    "#     Two dataframes are used to graph categorical changes across two datasets and\n",
    "#     two time periods. An example of this is postings within the post-doc bucket being\n",
    "#     comapared to postings in HEJP as a whole in 2007 and 2019.\n",
    "def super_graph_growth(df1, df2, category, years, title, type1, type2, colors1, colors2, scale1=1, scale2=1, top=10):\n",
    "    '''\n",
    "    Don't worry about this method. This is simply matplot code to generate the visual \n",
    "    for your convenience.\n",
    "    \n",
    "    If you choose to fiddle with it, do so by modifying the input arguments of the function.\n",
    "    '''\n",
    "    \n",
    "    comb = df1.merge(df2, on='index', how='inner')\n",
    "    comb = comb.sort_values(by='growth_y', ascending=False)\n",
    "    comb = comb[:top]\n",
    "    \n",
    "#     display(comb)\n",
    "    \n",
    "    labels = comb['index'].values\n",
    "    ind = np.arange(len(labels))\n",
    "    width = 0.4\n",
    "    \n",
    "    first1 = comb['count_x_x'].values/scale1\n",
    "    first2 = comb['count_y_x'].values/scale1\n",
    "    \n",
    "    second1 = comb['count_x_y'].values/scale2\n",
    "    second2 = comb['count_y_y'].values/scale2\n",
    "    \n",
    "    top_y = max(max(np.amax(first1), np.amax(first2)), max(np.amax(second1), np.amax(second2))) * 1.1\n",
    "    \n",
    "    fig, ax2 = plt.subplots(figsize=(14, 6))\n",
    "    \n",
    "    bar3 = ax2.bar(ind-width/2, second1, width, color=colors2[0], label=str(years[0]) + ' ' + type2)\n",
    "    bar4 = ax2.bar(ind+width/2, second2, width, color=colors2[1], label=str(years[1]) + ' ' + type2)\n",
    "    \n",
    "    ax2.set_ylim(top=top_y)\n",
    "    ax2.set_ylabel('Number of ' + type2 + ' Jobs in ' + category + '\\nScale Factor: ' + str(scale2))\n",
    "    \n",
    "    ax1 = ax2.twinx()\n",
    "    ax1.grid(False)\n",
    "    \n",
    "    bar1 = ax1.bar(ind-width/2, first1, width/2, color=colors1[0], label=str(years[0]) + ' ' + type1)\n",
    "    bar2 = ax1.bar(ind+width/2, first2, width/2, color=colors1[1], label=str(years[1]) + ' ' + type1)\n",
    "    \n",
    "    ax1.set_ylim(top=top_y)\n",
    "    ax1.set_ylabel('Number of ' + type1 + ' Jobs in ' + category + '\\nScale Factor: ' + str(scale1))\n",
    "    \n",
    "    plt.xticks(ind, labels, rotation=45, ha='right')\n",
    "    plt.xlabel(category)\n",
    "    \n",
    "    bars = [bar1, bar3, bar2, bar4]\n",
    "    labs = [b.get_label() for b in bars]\n",
    "    plt.legend(bars, labs, loc='best')\n",
    "    \n",
    "    plt.title(title)\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "# Graph Post-Doc vs HEJP baseline superimposed\n",
    "super_graph_growth(zone_g, main_zone_g, 'BEA Zone', (year1, year2), f'Post-Doc BEA Zone Growth vs HEJP Baseline\\n{year1} to {year2}', \n",
    "                   'Post-Docs', 'HEJP', ('royalblue', 'goldenrod'), ('cornflowerblue', 'gold'), scale2=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################\n",
    "# Visualization 2)\n",
    "# Overall Categorical Size Changes 2007-2019\n",
    "############################################################  \n",
    "\n",
    "# Total Categorical Size Changes\n",
    "\n",
    "year1 = 2007\n",
    "year2 = 2019\n",
    "\n",
    "post = main_table.merge(mask, on='Job ID', how='inner')\n",
    "main = main_table.drop(post.index)\n",
    "\n",
    "post1 = post[post['Year']==year1]['Job ID'].nunique()\n",
    "main1 = main[main['Year']==year1]['Job ID'].nunique()\n",
    "\n",
    "post2 = post[post['Year']==year2]['Job ID'].nunique()\n",
    "main2 = main[main['Year']==year2]['Job ID'].nunique()\n",
    "\n",
    "\n",
    "table = pd.DataFrame([['Post-Doc', post1, post2], ['HEJP (No PD)', main1, main2]], columns=['Type', year1, year2])\n",
    "\n",
    "table['growth'] = np.true_divide(table[year2]-table[year1], table[year1])\n",
    "\n",
    "display(table)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "############################################################\n",
    "# Visualization 3 & 4)\n",
    "# Number of Post-Docs per region every year 2007-2019\n",
    "############################################################ \n",
    "                      \n",
    "post = main_table.merge(mask, on='Job ID', how='inner')\n",
    "post = post[~post['IPEDS Institution Name'].isnull()]\n",
    "years = [2007, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019]\n",
    "# Take the first 8 to avoid 'Territories'\n",
    "zones = list(post['Institution_BEA_zone'].unique())[:8]\n",
    "# zones = ['Plains', 'Mideast', 'New England']\n",
    "tensor = np.zeros((len(years), 3, 0))\n",
    "\n",
    "post = pd.DataFrame(post.groupby(['Institution_BEA_zone', 'Year', 'IPEDS Institution Name']).count()['Job ID']).reset_index()\n",
    "\n",
    "# This loop composes a 3d array that has this structure\n",
    "# (Year number, 1 of 3 metrics [Post-Doc counts, institution counts, post-docs per institution], BEA_zones)\n",
    "for zone in zones:\n",
    "    DF = post[post['Institution_BEA_zone']==zone]\n",
    "    tuples = np.zeros((0, 3))\n",
    "    for year in years:\n",
    "        df = DF[DF['Year']==year]\n",
    "        \n",
    "#         df = df[df['Job ID']>=6]\n",
    "    \n",
    "        jobs = df['Job ID'].sum()\n",
    "        inst = df['IPEDS Institution Name'].nunique()\n",
    "\n",
    "        per = jobs/inst\n",
    "\n",
    "        tuples = np.vstack((tuples, np.array((jobs, inst, per))))\n",
    "    \n",
    "    tensor = np.dstack((tensor, tuples))\n",
    "\n",
    "ind = np.arange(len(years))\n",
    "colors = ['royalblue', 'limegreen', 'firebrick', 'c', 'indigo', 'gold', 'deeppink', 'grey']\n",
    "titles = ['Post-Doc Counts', 'Number of Institutions', 'Post per Institution']\n",
    "\n",
    "# This makes line graphs of all three metrics, but in the presentation we only used the first in a line graph\n",
    "for i in range(3):\n",
    "    fig, ax = plt.subplots(figsize=(12,9))\n",
    "    for j in range(len(zones)):\n",
    "        ax.plot(tensor[:,i,j], color=colors[j], label=zones[j])\n",
    "    plt.title(titles[i])\n",
    "    plt.xticks(ind, years)\n",
    "    plt.legend(loc='best')\n",
    "    plt.show()\n",
    "    \n",
    "# Isolate Institution and per capita numbers in the most recent year (2019)\n",
    "df = pd.DataFrame(tensor[8,1:,:])\n",
    "\n",
    "# Make the columns readable\n",
    "for i in range(len(zones)):\n",
    "    df = df.rename(columns={i:zones[i]})\n",
    "\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################\n",
    "# Visualization 5)\n",
    "# Top States with postings outside their borders\n",
    "############################################################ \n",
    "\n",
    "post = main_table.merge(mask, on='Job ID', how='inner')\n",
    "\n",
    "cat1 = 'Institution_State'\n",
    "cat2 = 'State'\n",
    "\n",
    "# cat1 = 'Institution_BEA_zone'\n",
    "# cat2 = 'BEA_zone'\n",
    "\n",
    "# Get all combinations of institution state and state\n",
    "diff = pd.DataFrame(post.groupby(['Year', cat1, cat2]).count()[['Job ID']]).reset_index()\n",
    "# Throw out observations in the same state\n",
    "diff = diff[diff[cat1]!=diff[cat2]]\n",
    "\n",
    "# Regroup by the Institution state\n",
    "diff = pd.DataFrame(diff.groupby([cat1]).sum()).reset_index()[['Institution_State', 'Job ID']]\n",
    "\n",
    "diff.sort_values(by='Job ID', ascending=False)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "############################################################\n",
    "# Visualization 6)\n",
    "# Skill Changes backed in from 2019\n",
    "############################################################ \n",
    "\n",
    "year1 = 2010\n",
    "year2 = 2019\n",
    "\n",
    "# Obtain isolated post-doctoral skill dataframe\n",
    "skill_small = skill_table[(skill_table['Year']==year1)|(skill_table['Year']==year2)]\n",
    "\n",
    "skill_p = skill_small.merge(mask, on='Job ID', how='inner')\n",
    "# skill_f = skill_table.drop(columns=['Unnamed: 0', 'Unnamed: 0.1']).merge(faculty_table[faculty_table['Faculty']==1][['Job ID']], \n",
    "#                                                                          on='Job ID', how='inner')\n",
    "\n",
    "# Method for isolating the skills in a year\n",
    "def skill_break(df, year, NSF=None):\n",
    "    df = df[df['Year']==year]\n",
    "    s_df = df[df['Specialized']==1]\n",
    "    \n",
    "    if NSF is not None:\n",
    "        mask = faculty_table[faculty_table[NSF]==1][['Job ID']]\n",
    "        df = df.merge(mask, on='Job ID', how='inner')\n",
    "        s_df = s_df.merge(mask, on='Job ID', how='inner')\n",
    "    \n",
    "    skills = pd.DataFrame(df['Skill Name'].value_counts()).reset_index()\n",
    "    skills = skills.rename(columns={'Skill Name':'count'})\n",
    "    skills['inc'] = np.true_divide(skills['count'], df['Job ID'].nunique())\n",
    "    \n",
    "    s_skills = pd.DataFrame(s_df['Skill Name'].value_counts()).reset_index()\n",
    "    s_skills = s_skills.rename(columns={'Skill Name':'count'})\n",
    "    s_skills['inc'] = np.true_divide(s_skills['count'], s_df['Job ID'].nunique())\n",
    "    \n",
    "    return skills, s_skills\n",
    "\n",
    "# Method for graphing the raw ranks of the skills\n",
    "def s_graph_rank(df, title, color='blue', top=10):\n",
    "    \n",
    "    df = df[:top]\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(12,6))\n",
    "    \n",
    "    labels = df['index'].values\n",
    "    counts = df['count'].values\n",
    "    \n",
    "    ind = np.arange(len(labels))\n",
    "    \n",
    "    ax.bar(ind, counts, color=color)\n",
    "    \n",
    "    plt.xticks(ind, labels, rotation=45, ha='right')\n",
    "    plt.xlabel('Skill Name')\n",
    "    plt.ylabel('Number with Skill')\n",
    "#     plt.legend(loc='upper right')\n",
    "    plt.title(title)\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "# Define NSF fields of intrest\n",
    "nsfs = ['Economics', 'Chemistry', 'Biological_and_Biomedical_Sciences']\n",
    "colors = [('royalblue', 'cornflowerblue'),('maroon', 'firebrick'),('goldenrod', 'gold'),('darkgreen', 'forestgreen'),\n",
    "          ('indigo', 'rebeccapurple')]\n",
    "\n",
    "def s_graph_growth(df, title, color='blue', top=10):\n",
    "    \n",
    "    df = df[:top]\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(6,12))\n",
    "    \n",
    "    labels = df['index'].values\n",
    "    counts = df['change'].values\n",
    "    \n",
    "    ind = np.arange(len(labels))\n",
    "    \n",
    "    ax.barh(ind, counts, color=color)\n",
    "    \n",
    "    plt.yticks(ind, labels, rotation=45, ha='right')\n",
    "    plt.ylabel('change')\n",
    "    plt.xlabel('Change in Demand')\n",
    "#     plt.legend(loc='upper right')\n",
    "    plt.title(title)\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "def get_diffs(df):\n",
    "    df['change'] = df['inc_y'] - df['inc_x']\n",
    "    df['growth'] = np.true_divide(df['inc_y'] - df['inc_x'], df['inc_x'])\n",
    "    return df.sort_values(by='change', ascending=False)\n",
    "\n",
    "for i in range(len(nsfs)):\n",
    "    nsf = nsfs[i]\n",
    "    color = colors[i]\n",
    "    \n",
    "    print(nsf)\n",
    "    \n",
    "    # Faculty dataframes are commneted out\n",
    "    \n",
    "    skills1, s_skills1 = skill_break(skill_p, year1, NSF=nsf)\n",
    "#     main_skills1, main_s_skills1 = skill_break(skill_f, year1, NSF=nsf)\n",
    "\n",
    "    skills2, s_skills2 = skill_break(skill_p, year2, NSF=nsf)\n",
    "#     main_skills2, main_s_skills2 = skill_break(skill_f, year2, NSF=nsf)\n",
    "\n",
    "#     fac_s = main_s_skills1.merge(main_s_skills2[:10], on='index', how='right')\n",
    "#     fac_s = get_diffs(fac_s)\n",
    "    \n",
    "    post_s = s_skills1.merge(s_skills2[:10], on='index', how='right')\n",
    "    post_s = get_diffs(post_s)\n",
    "    \n",
    "    display(post_s)\n",
    "    \n",
    "    s_graph_rank(s_skills2, 'Top Ranked Skills for ' + nsf + f'\\nPost-Docs in {year2}', color=color[1])\n",
    "    s_graph_growth(post_s, 'Change in Demanding Percentage for Top\\n' + nsf + f' Post-Doc Skills in {year2}', color=color[1])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skill_table[skill_table['Skill Name'].str.contains('ethics', na=False, case=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = faculty_table[faculty_table['Other_Humanities_and_Arts']==1][['Job ID']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main = mask.merge(main_table, how='inner')\n",
    "\n",
    "print(len(mask))\n",
    "\n",
    "target_mask = main[main['Job Title'].str.contains('Ethics', na=False, case=False)|\n",
    "     main['Job Title'].str.contains('Philosophy', na=False, case=False)][['Job ID']]\n",
    "\n",
    "target_mask = pd.concat([target_mask, skill_table[skill_table['Skill Name'].str.contains(\n",
    "    'Ethics', case=False, na=False)][['Job ID']]]).drop_duplicates()\n",
    "\n",
    "target = main_table.merge(target_mask, on='Job ID', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skill_table.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
